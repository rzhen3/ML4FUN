{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dcfb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as scp\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2065b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = fetch_ucirepo(id = 109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70558e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malicacid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity_of_ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>0D280_0D315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
       "0      14.23       1.71  2.43               15.6        127           2.80   \n",
       "1      13.20       1.78  2.14               11.2        100           2.65   \n",
       "2      13.16       2.36  2.67               18.6        101           2.80   \n",
       "3      14.37       1.95  2.50               16.8        113           3.85   \n",
       "4      13.24       2.59  2.87               21.0        118           2.80   \n",
       "..       ...        ...   ...                ...        ...            ...   \n",
       "173    13.71       5.65  2.45               20.5         95           1.68   \n",
       "174    13.40       3.91  2.48               23.0        102           1.80   \n",
       "175    13.27       4.28  2.26               20.0        120           1.59   \n",
       "176    13.17       2.59  2.37               20.0        120           1.65   \n",
       "177    14.13       4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     0D280_0D315_of_diluted_wines  Proline  class  \n",
       "0                            3.92     1065      1  \n",
       "1                            3.40     1050      1  \n",
       "2                            3.17     1185      1  \n",
       "3                            3.45     1480      1  \n",
       "4                            2.93      735      1  \n",
       "..                            ...      ...    ...  \n",
       "173                          1.74      740      3  \n",
       "174                          1.56      750      3  \n",
       "175                          1.56      835      3  \n",
       "176                          1.62      840      3  \n",
       "177                          1.60      560      3  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_df = pd.concat([wine['data']['features'], wine['data']['targets']], axis = 1)\n",
    "display(wine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305ad4d",
   "metadata": {},
   "source": [
    "weights and biases are joined within one matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599b8f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing concat by columns\n",
    "test_weights = torch.ones((4, 5))\n",
    "display(test_weights)\n",
    "test_biases = torch.zeros((4, 1))\n",
    "display(test_biases)\n",
    "test_concat = torch.concat([test_weights, test_biases], dim = 1)\n",
    "display(test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be02cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing concat by rows\n",
    "test_weights = torch.ones((3, 4))\n",
    "display(test_weights)\n",
    "test_biases = torch.zeros((2, 4))\n",
    "display(test_biases)\n",
    "test_concat = torch.concat([test_weights, test_biases], dim = 0)\n",
    "display(test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d2b523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing stack by rows\n",
    "test_weights = torch.ones((3, 4))\n",
    "display(test_weights)\n",
    "test_biases = torch.zeros((3, 4))\n",
    "display(test_biases)\n",
    "test_concat = torch.stack([test_weights, test_biases], dim = 0)\n",
    "test_concat2 = torch.stack([test_weights, test_biases], dim = 2)\n",
    "display(test_concat)\n",
    "display(test_concat2)\n",
    "display(test_concat2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa14c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 13])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing matrix multiplication in pytorch\n",
    "\n",
    "# assume 50 samples, 12 features and we include a bias term\n",
    "X = torch.randn((50, 12))\n",
    "bias_ones = torch.ones((X.shape[0], 1))\n",
    "\n",
    "W = torch.randn((X.shape[1] + 1, 1))\n",
    "X_aug = torch.concat([bias_ones, X], dim = 1)\n",
    "\n",
    "display(X_aug.shape)\n",
    "display(X_aug[:, 0])\n",
    "\n",
    "y = X_aug @ W\n",
    "display(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc44d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def l2_loss(y_hat, y):\n",
    "#     loss = torch.sum((y_hat - y)**2)\n",
    "#     return loss\n",
    "\n",
    "def l1_loss(y_hat, y):\n",
    "    loss = torch.sum(torch.abs(y_hat - y))\n",
    "    # print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec05e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.0998, 2.6081, 2.1924, 3.2629, 2.2153],\n",
       "        [2.4403, 1.4220, 3.6794, 1.8607, 3.8025],\n",
       "        [1.9022, 2.4286, 4.1610, 3.1501, 0.9110],\n",
       "        [2.8524, 2.6757, 4.4514, 2.9013, 2.1566]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.3237, 2.2836, 3.6211, 2.7937, 2.2714])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y1 = torch.ones((5, 1))\n",
    "y2 = torch.ones((5, 1)) * 3\n",
    "\n",
    "# display(torch.sum(y2))\n",
    "l1_loss(y1, y2)\n",
    "display(torch.mean(y1))\n",
    "\n",
    "yn = torch.normal(3., 1., (4, 5))\n",
    "display(yn)\n",
    "display(torch.mean(yn, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b947c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3653445817.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mW_aug =\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, num_features, num_classes, loss_fn):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.W = torch.normal(0, 0.01, (self.num_features, self.num_classes), dtype = torch.float64)\n",
    "        self.b = torch.zeros(self.num_classes, dtype = torch.float64)       # could try an array of biases to see if it helps\n",
    "\n",
    "    def fit(self, X, y, epochs = 1000, lr = 0.01, ):\n",
    "        # enforce right dimensions\n",
    "        assert X.shape[1] == self.num_features and y.shape[1] == self.num_classes\n",
    "        \n",
    "        # computes the gradient descent version of linear regression\n",
    "        for t in range(epochs):\n",
    "            y_hat = self.forward(X)\n",
    "            loss = self.loss_fn(X, y)\n",
    "\n",
    "            # compute gradient\n",
    "            dW = (2/X.shape[0]) * (X.T @ (y - y_hat))\n",
    "            db = (2/X.shape[0]) * torch.sum(y - y_hat)\n",
    "\n",
    "            # update features\n",
    "            self.W = self.W - (lr * dW)\n",
    "            self.b = self.b - (lr * db)\n",
    "\n",
    "\n",
    "    def fit_OLS(self, X, y):\n",
    "        assert X.shape[1] == self.num_features and y.shape[1] == self.num_classes\n",
    "\n",
    "        # compute OLS solution\n",
    "        W_OLS = torch.inverse(X.T @ X) @ X.T @ y\n",
    "        b_OLS = torch.mean(y) - torch.mean(X @ W)\n",
    "        self.W = W_OLS\n",
    "        self.b = b_OLS\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # assert X is of the shape (num_inputs, num_features)\n",
    "        assert X.shape[1] == self.num_features\n",
    "        return (X @ self.W) + self.b\n",
    "    \n",
    "    def predict_proba(self, X_val):\n",
    "        if self.num_classes == 1:\n",
    "            # perform binary classification\n",
    "        else:\n",
    "            # perform multiclassification\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_val):\n",
    "        # also split into 2 cases for binary- and multi- classification\n",
    "\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22062fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
