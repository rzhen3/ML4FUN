{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59dcfb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as scp\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2065b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = fetch_ucirepo(id = 109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70558e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malicacid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity_of_ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>0D280_0D315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
       "0      14.23       1.71  2.43               15.6        127           2.80   \n",
       "1      13.20       1.78  2.14               11.2        100           2.65   \n",
       "2      13.16       2.36  2.67               18.6        101           2.80   \n",
       "3      14.37       1.95  2.50               16.8        113           3.85   \n",
       "4      13.24       2.59  2.87               21.0        118           2.80   \n",
       "..       ...        ...   ...                ...        ...            ...   \n",
       "173    13.71       5.65  2.45               20.5         95           1.68   \n",
       "174    13.40       3.91  2.48               23.0        102           1.80   \n",
       "175    13.27       4.28  2.26               20.0        120           1.59   \n",
       "176    13.17       2.59  2.37               20.0        120           1.65   \n",
       "177    14.13       4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     0D280_0D315_of_diluted_wines  Proline  class  \n",
       "0                            3.92     1065      1  \n",
       "1                            3.40     1050      1  \n",
       "2                            3.17     1185      1  \n",
       "3                            3.45     1480      1  \n",
       "4                            2.93      735      1  \n",
       "..                            ...      ...    ...  \n",
       "173                          1.74      740      3  \n",
       "174                          1.56      750      3  \n",
       "175                          1.56      835      3  \n",
       "176                          1.62      840      3  \n",
       "177                          1.60      560      3  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_df = pd.concat([wine['data']['features'], wine['data']['targets']], axis = 1)\n",
    "display(wine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6305ad4d",
   "metadata": {},
   "source": [
    "weights and biases are joined within one matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599b8f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing concat by columns\n",
    "test_weights = torch.ones((4, 5))\n",
    "display(test_weights)\n",
    "test_biases = torch.zeros((4, 1))\n",
    "display(test_biases)\n",
    "test_concat = torch.concat([test_weights, test_biases], dim = 1)\n",
    "display(test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be02cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing concat by rows\n",
    "test_weights = torch.ones((3, 4))\n",
    "display(test_weights)\n",
    "test_biases = torch.zeros((2, 4))\n",
    "display(test_biases)\n",
    "test_concat = torch.concat([test_weights, test_biases], dim = 0)\n",
    "display(test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d2b523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing stack by rows\n",
    "test_weights = torch.ones((3, 4))\n",
    "display(test_weights)\n",
    "test_biases = torch.zeros((3, 4))\n",
    "display(test_biases)\n",
    "test_concat = torch.stack([test_weights, test_biases], dim = 0)\n",
    "test_concat2 = torch.stack([test_weights, test_biases], dim = 2)\n",
    "display(test_concat)\n",
    "display(test_concat2)\n",
    "display(test_concat2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa14c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 13])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing matrix multiplication in pytorch\n",
    "\n",
    "# assume 50 samples, 12 features and we include a bias term\n",
    "X = torch.randn((50, 12))\n",
    "bias_ones = torch.ones((X.shape[0], 1))\n",
    "\n",
    "W = torch.randn((X.shape[1] + 1, 1))\n",
    "X_aug = torch.concat([bias_ones, X], dim = 1)\n",
    "\n",
    "display(X_aug.shape)\n",
    "display(X_aug[:, 0])\n",
    "\n",
    "y = X_aug @ W\n",
    "display(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ec05e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.9792, 2.3095, 2.9816, 2.3606, 1.4868],\n",
       "        [2.7203, 2.9152, 4.5532, 5.2039, 3.3663],\n",
       "        [4.2312, 5.0009, 4.4857, 3.5247, 3.8831],\n",
       "        [3.4758, 4.4499, 3.0263, 2.7034, 3.6818]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.3516, 3.6689, 3.7617, 3.4482, 3.1045])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y1 = torch.ones((5, 1))\n",
    "y2 = torch.ones((5, 1)) * 3\n",
    "\n",
    "# display(torch.sum(y2))\n",
    "l1_loss(y1, y2)\n",
    "display(torch.mean(y1))\n",
    "\n",
    "yn = torch.normal(3., 1., (4, 5))\n",
    "display(yn)\n",
    "display(torch.mean(yn, dim = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49db972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6782, -0.5683,  1.4735],\n",
       "        [ 1.5356,  1.9042,  0.5734],\n",
       "        [-1.0408,  0.5293,  1.1582],\n",
       "        [ 0.0414, -0.2379,  0.3414],\n",
       "        [ 1.8596, -0.4059,  0.5144],\n",
       "        [ 1.1634,  0.5480, -0.5610],\n",
       "        [-2.5391, -0.7623, -0.5073],\n",
       "        [ 1.0764,  0.1422,  1.5119],\n",
       "        [ 0.1130,  2.0296, -1.8854],\n",
       "        [ 1.4712, -1.4557,  0.6822]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0933, 0.1042, 0.8025],\n",
       "        [0.3536, 0.5113, 0.1351],\n",
       "        [0.0675, 0.3243, 0.6082],\n",
       "        [0.3219, 0.2435, 0.4346],\n",
       "        [0.7330, 0.0761, 0.1909],\n",
       "        [0.5818, 0.3144, 0.1037],\n",
       "        [0.0688, 0.4066, 0.5246],\n",
       "        [0.3403, 0.1337, 0.5260],\n",
       "        [0.1260, 0.8569, 0.0171],\n",
       "        [0.6632, 0.0355, 0.3013]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5645],\n",
       "        [0.7243],\n",
       "        [0.6460],\n",
       "        [0.6762],\n",
       "        [0.4205]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]], dtype=torch.int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6974, -2.0676, -1.2959],\n",
       "        [ 0.7992,  1.8889,  1.5988],\n",
       "        [ 0.0589, -0.3988,  0.1046],\n",
       "        [-1.3218,  0.3301,  0.3015],\n",
       "        [ 0.2312, -0.0886, -0.3455],\n",
       "        [ 1.8234,  0.4439,  0.0927],\n",
       "        [-0.7024, -0.7592,  0.0045],\n",
       "        [-0.2246,  0.9885, -1.0802],\n",
       "        [-0.4810, -1.2491, -0.4484],\n",
       "        [ 1.7724,  0.0319, -1.2149]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# testing out torch.softmax\n",
    "yhat = torch.randn(10, 3)\n",
    "display(yhat)\n",
    "sftmx = torch.softmax(yhat, dim = 1)\n",
    "display(sftmx)\n",
    "\n",
    "# testing out conditionals for tensors\n",
    "yprobs = torch.sigmoid(torch.randn(5, 1))\n",
    "display(yprobs)\n",
    "preds = (yprobs > 0.5).int()\n",
    "display(preds)\n",
    "\n",
    "test = torch.randn(10, 3)\n",
    "display(test)\n",
    "t = torch.argmax(test, dim = 1).reshape((-1, 1))\n",
    "display(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def l2_loss(y_hat, y):\n",
    "#     loss = torch.sum((y_hat - y)**2)\n",
    "#     return loss\n",
    "\n",
    "def l1_loss(y_hat, y):\n",
    "    loss = torch.sum(torch.abs(y_hat - y))\n",
    "    # print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b947c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3653445817.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mW_aug =\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, num_features, num_classes, loss_fn, get_grad):\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_fn = loss_fn\n",
    "        self.get_grad = get_grad\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.W = torch.normal(0, 0.01, (self.num_features, self.num_classes), dtype = torch.float64)\n",
    "        self.b = torch.zeros(self.num_classes, dtype = torch.float64)       # could try an array of biases to see if it helps\n",
    "\n",
    "    def fit(self, X, y, epochs = 1000, lr = 0.01, ):\n",
    "        # enforce right dimensions\n",
    "        assert X.shape[1] == self.num_features and y.shape[1] == self.num_classes\n",
    "        \n",
    "        # computes the gradient descent version of linear regression\n",
    "        for t in range(epochs):\n",
    "            y_hat = self.forward(X)\n",
    "            loss = self.loss_fn(X, y)\n",
    "\n",
    "            # compute gradient\n",
    "            # dW = (2/X.shape[0]) * (X.T @ (y - y_hat))\n",
    "            # db = (2/X.shape[0]) * torch.sum(y - y_hat)\n",
    "            dW, db = self.get_grad(X, y, y_hat)\n",
    "\n",
    "            # update features\n",
    "            self.W = self.W - (lr * dW)\n",
    "            self.b = self.b - (lr * db)\n",
    "\n",
    "\n",
    "    def fit_OLS(self, X, y):\n",
    "        assert X.shape[1] == self.num_features and y.shape[1] == self.num_classes\n",
    "\n",
    "        # compute OLS solution\n",
    "        W_OLS = torch.inverse(X.T @ X) @ X.T @ y\n",
    "        b_OLS = torch.mean(y) - torch.mean(X @ W)\n",
    "        self.W = W_OLS\n",
    "        self.b = b_OLS\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # assert X is of the shape (num_inputs, num_features)\n",
    "        assert X.shape[1] == self.num_features\n",
    "        return (X @ self.W) + self.b\n",
    "    \n",
    "    def predict_proba(self, X_val):\n",
    "        return torch.sigmoid(self.forward(X_val)) \\\n",
    "            if self.num_classes == 1 \\\n",
    "                else torch.softmax(self.forward(X_val), dim = 1, dtype=float)\n",
    "    def predict(self, X_val):\n",
    "        # also split into 2 cases for binary- and multi- classification\n",
    "        if self.num_classes == 1:\n",
    "            # binary classification\n",
    "            preds = self.predict_proba(X_val)\n",
    "            return (preds > 0.5).int()\n",
    "        else:\n",
    "            # multiclassification\n",
    "            preds = self.predict_proba(X_val)\n",
    "            return torch.argmax(preds, dim = 1).reshape((-1, 1))    # could get rid of the reshape\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22062fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, num_features, loss_fn, get_grad):\n",
    "        self.num_features = num_features\n",
    "        self.loss_fn = loss_fn\n",
    "        self.get_grad = get_grad\n",
    "\n",
    "        self.init_weights\n",
    "    def init_weights(self):\n",
    "        self.W = torch.normal(0, 1, size = (self.num_features, 1))\n",
    "        self.b = 0.\n",
    "\n",
    "    def fit(self, X, y, epochs, lr):\n",
    "        assert X.shape[1] == self.num_features\n",
    "\n",
    "        for epoch in epochs:\n",
    "            y_hat = self.forward(X)\n",
    "\n",
    "            # compute loss\n",
    "            loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "            # compute gradient\n",
    "            # dW = 2 * (X.T @ (y_hat - y)) / X.shape[0]\n",
    "            # db = 2 * torch.sum(y_hat - y) / X.shape[0]\n",
    "            dW, db = self.get_grad(X, y, y_hat)\n",
    "\n",
    "            # update weights\n",
    "            self.W = self.W - (lr * dW)\n",
    "            self.b = self.b - (lr * db)\n",
    "\n",
    "    def fit_OLS(self, X, y):\n",
    "        \n",
    "        W_OLS = torch.inverse(X.T @ X) @ X.T @ y\n",
    "        b_OLS = torch.mean(y) - torch.mean(X @ W_OLS)\n",
    "\n",
    "        self.W = W_OLS\n",
    "        self.b = b_OLS\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.sigmoid(X @ self.W + self.b)\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.forward(X) > 0.5).int()\n",
    "    \n",
    "    def verify_grad(self, threshold):\n",
    "        epsilon = 1e-5\n",
    "\n",
    "        wd = self.W[0]\n",
    "        w_minus_d = self.W[1:]\n",
    "        wpos = wd + epsilon\n",
    "        wneg = wd - epsilon\n",
    "\n",
    "        approx_dw = (self.loss_fn(wpos, w_minus_d) - self.loss_fn(wneg, w_minus_d))/(2*epsilon)\n",
    "\n",
    "        analytical_dw = self.get_grad()\n",
    "\n",
    "        result = ((analytical_dw - approx_dw)**2)/((analytical_dw + approx_dw)**2)\n",
    "\n",
    "        return result < threshold\n",
    "        \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
